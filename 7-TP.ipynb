{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'gymnasium[classic-control]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6630512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86b039",
   "metadata": {},
   "source": [
    "<img src=\"https://stanford-cs221.github.io/autumn2023/assignments/mountaincar/mountaincar.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c6133",
   "metadata": {},
   "source": [
    "On considère l'environnement *Mountain Car* où on contrôle une voiture dans une vallée avec pour but d'atteindre le sommet de droite. On dispose de 3 actions: accélérer vers la gauche (0), ne pas accélerer (1), accélérer vers la droite (2). Un état correspond à un couple $(m,v)\\in \\left[ - 1.2,.6 \\right] \\times \\left[ - .07 ,.07 \\right]$ correspondant à la position (l'abscisse) et la vitesse. L'état de départ d'une interaction (\"épisode\") est choisi aléatoirement mais toujours dans la vallée.\n",
    "\n",
    "On pourra consulter la documentation officielle pour plus d'informations : https://gymnasium.farama.org/environments/classic_control/mountain_car/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff66da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e691068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(policy):\n",
    "    local_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "    s = local_env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        a = policy(s)\n",
    "        s, r, terminated, truncated , _ = local_env.step(a)\n",
    "        clear_output(wait=True)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(local_env.render())\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c361ff",
   "metadata": {},
   "source": [
    "La fonction `render_policy` ci-dessus permet de visualiser une politique durant un épisode, sous la forme d'une animation. L'argument `policy` doit être une fonction prenant en argument un état et renvoyant un action (choisie éventuellement de façon aléatoire).\n",
    "\n",
    "*Question 1* : Visualiser la politique accélérant toujours vers la droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816e4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab915a10",
   "metadata": {},
   "source": [
    "*Question 2* : Proposer une meilleure politique et la visualiser. Estimer la longueur moyenne d'un épisode lorsque cette politique est employée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b06145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bedeba",
   "metadata": {},
   "source": [
    "Pour le reste du TP, on oublie la description de l'environnement ainsi que l'intuition qu'on en a. On suppose qu'on a seulement la connaissance de l'ensemble des actions, de l'ensemble des états, et qu'on peut iteragir sans limite avec l'envionnement dans le cadre d'épisodes dont on ne choisit pas l'état initial.\n",
    "\n",
    "L'ensemble d'états étant infini (ou en toute rigueur, fini mais de cardinal très élevé, si on considère que les valeurs sont de type float32), on souhaite construire un algorithme d'apprentissage par renforcement s'appuyant sur une famille paramétrique de fonctions valeur. \n",
    "\n",
    "On fait le choix d'une paramétrisation linéaire de fonctions\n",
    "action-valeur en dimension 1200 $(q_w)_{w\\in \\mathbb{R}^{1200}}$:\n",
    "$ q_w(s,a)=\\phi(s,a)^{\\top}\\!w,$\n",
    "où la *feature map* $\\phi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^{1200}$\n",
    "est construite de la façon suivante à l'aide d'une application\n",
    "$\\psi:\\mathcal{S}\\to \\mathbb{R}^{400}$:\n",
    "\n",
    "$ \\phi(s,0)=(\\psi(s)|0_{\\mathbb{R}^{800}})^{\\top} $\n",
    "\n",
    "$ \\phi(s,1)=(0_{\\mathbb{R}^{400}}|\\psi(s)|0_{\\mathbb{R}^{400}})^{\\top} $\n",
    "\n",
    "$ \\phi(s,2)=(0_{\\mathbb{R}^{800}}|\\psi(s))^{\\top} $\n",
    "\n",
    "On construit la fonction $\\psi$ de la façon suivante,\n",
    "en effectuant d'abord une normalisation (sur la base de 10000 exemples de valeurs d'état), puis en utilisant la fonction `RBFSampler` de `scikit-learn` qui permet d'approximer en dimension finie la `feature map` d'un noyau gaussien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b636e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to convert a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "def psi(s):\n",
    "    return np.squeeze(featurizer.transform(scaler.transform(s[np.newaxis,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa10aa5",
   "metadata": {},
   "source": [
    "*Question 3* : Compléter les fonctions suivantes qui calculent $\\phi(s,a)$ et $q_w(s,a)$ selon les définitions données plus haut. *On pourra faire appel à la fonction `np.concatenate`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f4b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(s,a):\n",
    "    # compléter\n",
    "\n",
    "def q(s,a,w):\n",
    "    # compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6014a",
   "metadata": {},
   "source": [
    "*Question 4* : En s'inspirant des TP précédents, écrire une fonction `draw_action_greedy_policy` qui tire une action selon une politique $\\epsilon$-gloutonne par rapport à une fonction action-valeur $q_w$ de la famille paramétrique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949a8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action_greedy_policy(s, w, eps=0):\n",
    "    # compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddcba8",
   "metadata": {},
   "source": [
    "*Question 5* : Implémenter un Q-learning (semi-gradient). Essayer plusieurs valeurs pour $\\alpha$. Visualiser la politique obtenue au bout de 10 épisodes (puis 20, 50, 100, 200 épisodes). Tracer l'évolution de la longueur des épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073dbf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10047 @ Episode 10/10 (-329.0)"
     ]
    }
   ],
   "source": [
    "gamma = .99\n",
    "w = np.zeros(400*3)\n",
    "n_episodes = 10\n",
    "k = 0\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    s = env.reset()[0]\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    # on ignore le signal de troncation, afin d'aller au-delà de la limite des 200 étapes par épisode\n",
    "    while not terminated:\n",
    "        # compléter\n",
    "\n",
    "        print(\"\\rStep {} @ Episode {}/{}\".format(k, episode + 1, n_episodes), end=\"\")\n",
    "        k += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0acb5",
   "metadata": {},
   "source": [
    "*Question 6* : Même question avec SARSA (à n étapes) (semi-gradient)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "name": "7-TP.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
