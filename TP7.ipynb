{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468f3424-91b2-47b2-b06b-6207b24035d2",
   "metadata": {},
   "source": [
    "On travaille avec l'environnement CliffWalking de Gymnasium (voir la documentation https://gymnasium.farama.org/environments/toy_text/cliff_walking/). On dispose de quatre actions (0, 1, 2, 3), ainsi que de 48 états (0, 1, ..., 47). L'état initial d'un épisode est toujours 36, et un épisode se termine lorsque l'état 47 est atteint. Le but du TP est d'implémenter un algorithme acteur-critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfb9d8-95fc-47c8-b19a-d3ea0ac5979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'gymnasium[toy_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8098fe-4434-4b9c-85f7-a5b6b66aab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246ba1f-abe8-449a-b7b0-4dd9b99c01da",
   "metadata": {},
   "source": [
    "*Question 1*: Implémenter la fonction \"softmax\" qui à un vecteur $x\\in \\mathbb{R}^d$ associe le vecteur $\\displaystyle \\left(\\frac{\\exp\\left( x_i \\right) }{\\sum_{j=1}^d\\exp\\left( x_j \\right) }  \\right)_{1\\leqslant i\\leqslant n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8e1404-6000-467f-ad58-df5cfd29b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    # Compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084594fe-525f-426f-848e-71549e7a3ff7",
   "metadata": {},
   "source": [
    "On considère pour les politiques la classe paramétrique $\\left\\{\n",
    "\\pi_{\\theta} \\right\\}_{\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times\n",
    "\\mathcal{A}}}$ où $\\displaystyle \\pi_{\\theta}(\\,\\cdot\\,|s)=\\operatorname{softmax}\\left( (\\theta_{s,a})_{a\\in \\mathcal{A}} \\right)$.\n",
    "\n",
    "*Question 2*: Pour $\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$, $s\\in\n",
    "\\mathcal{S}$ et $a\\in \\mathcal{A}$, calculer $\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)$. Implémenter le calcul de ce gradient. On représentera un vecteur $\\theta\\in \\mathcal{S}\\times \\mathcal{A}$\n",
    "par un array NumPy de taille 48x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ef2589-2ea0-4ea2-813e-0c2a71ce3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_pi(s,a,theta):\n",
    "    # Compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5490d3e-8f40-478d-a0aa-d00da9d7a445",
   "metadata": {},
   "source": [
    "*Question 3*: Programmer une fonction qui pour $\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ et $s\\in\n",
    "\\mathcal{S}$ donnés, tire une action selon $\\pi_{\\theta}(\\,\\cdot\\,|s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7efc0a-4090-4949-a772-be548271ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action_pi(s, theta):\n",
    "    # on pourra utiliser la fonction np.random.choice\n",
    "    # Compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da801350-14d1-4f36-a178-b505d01592e9",
   "metadata": {},
   "source": [
    "Pour la *baseline*, on considère la classe paramétrique \"tabulaire\" $(v_w)_{w\\in\n",
    "\\mathbb{R}^{\\mathcal{S}}}$ où pour $w\\in \\mathbb{R}^\\mathcal{S}$ et\n",
    "$s\\in \\mathcal{S}$, $v_w(s)=w_s$.\n",
    "\n",
    "*Question 4*: Compléter les fonctions suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1955d0ab-4ade-4dcc-9250-40dffbe443a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v(s,w):\n",
    "    # Compléter\n",
    "\n",
    "def grad_v(s,w):\n",
    "    # Compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e6148-f169-4748-9206-59b5268e1eb6",
   "metadata": {},
   "source": [
    "*Question 5*: Implémenter l'algorithme acteur-critique du cours. Pour chaque exécution de l'algorithme, on tracera l'évolution de la longeur des épisodes. Observer les performances en essayant différentes valeurs pour les *pas* (alias *learning rate*). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6838c-e2ae-4c7b-854c-795959b19901",
   "metadata": {},
   "source": [
    "Dans sa version la plus basique, l'algorithme acteur-critique utilise la descent de gradient stochastique (SGD) pour la mise à jour des paramètres $\\theta$ et $w$, SGD s'écrivant de façon générale $x_{t+1}=x_t-\\alpha \\hat{g}_t$ où $\\hat{g}_t$ est un estimateur stochastique du gradient évalué en $x_t$ de la fonction à minimiser. Il existe des variantes, comme par exemple l'algorithme Adam, très utilisé en Deep Learning, qui s'écrit composante par composante : \n",
    "$\\displaystyle x_{t+1,i}=x_{t,i}-\\frac{\\eta}{\\varepsilon+\\sqrt{\\sum_{s=1}^t\\beta_2^{t-s}\\hat{g}_{s,i}^2}}\\sum_{s=1}^t\\beta_1^{t-s}\\hat{g}_{s,i},\\quad 1\\leqslant i\\leqslant n.$\n",
    "\n",
    "où on peut prendre par exemple $\\varepsilon=10^{-7}$, $\\beta_1=0.9$ et $\\beta_2=0.999$.\n",
    "\n",
    "*Question 6*: Modifier l'algorithme en remplaçant SGD par Adam. Comparer les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20a221e-fc4b-4e6d-9e4d-1b922c4e502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18465 @ Episode 1000/1000 (length 13)"
     ]
    }
   ],
   "source": [
    "episode_lengths = []\n",
    "n_episodes = 1000\n",
    "theta = np.zeros((48,4))\n",
    "w = np.zeros(48)\n",
    "k = 0\n",
    "gamma = 1\n",
    "\n",
    "# Compléter\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    s = env.reset()[0]\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    episode_lengths.append(0)\n",
    "    while not terminated:\n",
    "\n",
    "        a = draw_action_pi(s, theta)\n",
    "        s_, r, terminated, _, _ = env.step(a)\n",
    "\n",
    "        # Compléter\n",
    "\n",
    "        episode_lengths[-1] += 1\n",
    "        k += 1\n",
    "\n",
    "    print(\"\\rStep {} @ Episode {}/{} (length {})\".format(k, episode + 1, n_episodes, episode_lengths[-1]), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af39eb-4a94-41b9-b01d-749347db887c",
   "metadata": {},
   "source": [
    "*Question 7*: Implémenter REINFORCE (avec et sans baseline). Comparer les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132ddd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
