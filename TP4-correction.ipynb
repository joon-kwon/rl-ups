{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e81bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# On définit les ensembles S et A de sorte à pouvoir itérer dessus.\n",
    "S = []\n",
    "for current_sum in range(4,22):\n",
    "    for dealers_one_card in range(1,11):\n",
    "        S.append((current_sum,dealers_one_card, False))\n",
    "        if current_sum > 11:\n",
    "            S.append((current_sum,dealers_one_card, True))\n",
    "A = [0,1]\n",
    "\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0074b",
   "metadata": {},
   "source": [
    "Le package gymnasium fournit des environnements (MDP) avec lesquel on peut interagir, et donc faire de l'apprentissage par renforcement.\n",
    "\n",
    "On considère ici l'environnement Blackjack, qu'on peut décrire de la façon suivante. Un joueur joue contre un dealer. Le but du jeu est de faire en sorte que la somme de ses cartes soit la plus grande, mais sans dépasser 21. Les cartes J, Q, K valent 10. A peut valoir 1 ou 11, selon ce qui est le plus avantageux. \n",
    "- Au début du  jeu (d'un \"épisode\"), 2 cartes visibles sont distribuées au joueur, et 2 au dealer dont une est cachée.\n",
    "- A chaque étape, le joueur choisit Hit (demander une carte supplémentaire) ou Stand (en rester là)\n",
    "- Lorsque le joueur a terminé, le dealer se tire des cartes supplémentaires jusqu'à avoir au moins 17.\n",
    "\n",
    "Consulter la documentation (https://gymnasium.farama.org/environments/toy_text/blackjack/) pour savoir comment le problème est modélisé.\n",
    "\n",
    "Un environnement gymnasium fonctionne de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb40084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 4, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# démarre un nouvel épisode. Un état est tiré au hasard.\n",
    "s, _ = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defc03f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvel état (17, 4, 1)\n",
      "Paiement -1.0\n"
     ]
    }
   ],
   "source": [
    "# si l'agent choisir l'action a=0, un nouvel état s est tiré, et un paiement r est obtenu\n",
    "\n",
    "a = 0\n",
    "s, r, terminated, truncated, _ = env.step(a)\n",
    "print('Nouvel état', s)\n",
    "print('Paiement', r)\n",
    "\n",
    "# les variables terminated (resp. truncated) est un booléen qui est True lorsque l'épisode s'est achevé,\n",
    "# ce qui revient à dire que le paiement ne pourra dorénavant être que nul\n",
    "# (resp. lorsque l'épisode a été interrompu car le nombre d'étapes a atteint une limite fixée)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ad216",
   "metadata": {},
   "source": [
    "**Question 1**: Proposer une politique simple sous la forme d'une fonction qui prend un état en entrée et qui renvoie une action (0 ou 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46f0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(s):\n",
    "    own_sum, dealer_visible_card, useable_ace = s\n",
    "    if own_sum < min(dealer_visible_card*2,14):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53de6c",
   "metadata": {},
   "source": [
    "**Question 2**: Pour la politique précédente, évaluer la quantité\n",
    "$$\\mathbb{E}_{\\mu,\\pi}\\left[ \\sum_{t=1}^{+\\infty}R_t \\right]$$\n",
    "où $\\mu$ est la distribution initiale d'états, en calculant la moyenne des paiements sur 100000 parties (\"épisodes\") (On considère donc ici $\\gamma=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0af2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward -0.10449\n"
     ]
    }
   ],
   "source": [
    "cumul = 0\n",
    "n_iter = 100000\n",
    "for k in range(n_iter):\n",
    "    s, _ = env.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        s, r, terminated, truncated, _ = env.step(pi(s))\n",
    "        cumul += r\n",
    "print('mean reward', cumul/n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb28c9",
   "metadata": {},
   "source": [
    "**Question 3**: Définir une fonction qui prend en argument un état, une fonction action-valueur q (donnée sous la forme d'un dictionnaire dont les clés sont les états), ainsi qu'un epsilon, et qui avec probabilité epsilon renvoie une action tirée uniformément, et qui avec probabilité 1-epsilon renvoie une action choisie par une politique gloutonne par rapport à q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b64e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_eps_g(s,q,eps):\n",
    "    if np.random.uniform() < eps or q[(s,0)] == q[(s,1)]:\n",
    "        return np.random.randint(0,2)\n",
    "    else:\n",
    "        return 0 if q[(s,0)] > q[(s,1)] else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91ea55",
   "metadata": {},
   "source": [
    "**Question 4**: Définir une fonction qui prend en argument une fonction action-valeur q et un epsilon, qui génère un épisode en utilisant la politique définie à la question précédente, et qui renvoie 3 listes, contenants les états, les actions et les paiements obtenus pendant l'épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a8343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(q,eps):\n",
    "    ss = []\n",
    "    aa = []\n",
    "    rr = []\n",
    "\n",
    "    s, _ = env.reset()\n",
    "    ss.append(s)\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        a = pi_eps_g(s,q,eps)\n",
    "        aa.append(a)\n",
    "\n",
    "        s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        ss.append(s)\n",
    "        rr.append(r)\n",
    "\n",
    "    return ss, aa, rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f9df4",
   "metadata": {},
   "source": [
    "**Question 5**: On cherche à mettre en oeuvre une itération de politique approchée. A chaque itération, la fonction état-valeur de la politique actuelle est estimée en générant un très grand nombre d'épisodes en utilisant non pas exactement la vraie politique gloutonne, mais celle correspond à la fonction `pi_esp_g` (pour une valeur de epsilon à choisir). Pour chaque paire (s,a) visitée, la somme des paiements des étapes futurs (de l'épisode concerné) est enregistrée, puis à la fin de l'itération, la composante correspondante de la fonction état-valeur est estimée par la moyenne des valeurs enregistrées. Expliciter la politique finalement obtenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a09d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_and_N():\n",
    "    q = dict()\n",
    "    N = dict() \n",
    "    for s in S:\n",
    "        for a in A:\n",
    "            q[(s,a)] = 1.\n",
    "            N[(s,a)] = 0 # pour stocker le nombre de valeurs enregistrée pour chaque paire (s,a)\n",
    "    return q, N\n",
    "\n",
    "eps = .1\n",
    "n_iter = 30\n",
    "n_episodes_per_iter = 500000\n",
    "previous_q, _ = initialize_q_and_N()\n",
    "for iter in range(n_iter):\n",
    "    print('iter', iter)\n",
    "    current_cumul_q, current_N = initialize_q_and_N()\n",
    "    for episode in range(n_episodes_per_iter):\n",
    "        ss, aa, rr = generate_episode(previous_q,eps)\n",
    "        for t in range(len(rr)):\n",
    "            s = ss[t]\n",
    "            a = aa[t]\n",
    "            cumul_r = sum(rr[t:])\n",
    "            current_cumul_q[(s,a)] = cumul_r+current_cumul_q[(s,a)]\n",
    "            current_N[(s,a)] = 1 + current_N[(s,a)]\n",
    "\n",
    "    if 0 in current_N.values():\n",
    "        print('break')\n",
    "        break\n",
    "\n",
    "    previous_q = dict()\n",
    "    for s in S:\n",
    "        for a in A:\n",
    "            previous_q[(s,a)] = current_cumul_q[(s,a)]/current_N[(s,a)]\n",
    "            \n",
    "# estimate average reward\n",
    "cumul = 0\n",
    "n_iter = 100000\n",
    "for k in range(n_iter):\n",
    "    s, _ = env.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        s, r, terminated, truncated, _ = env.step(pi_eps_g(s,previous_q,eps))\n",
    "        cumul += r\n",
    "print('mean reward', cumul/n_iter)\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    if previous_q[(s,0)] >= previous_q[(s,1)]:\n",
    "        print(s,'Stick')\n",
    "    else:\n",
    "        print(s,'Hit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
