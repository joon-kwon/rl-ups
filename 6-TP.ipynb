{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90719276-0945-4548-aa48-c4a9ec6ad049",
   "metadata": {},
   "source": [
    "On fait appel dans ce TP au package Gymnasium qui implémente de nombreux environnements sous forme de MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb84cd-5943-4ab1-a920-fe12334a8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'gymnasium[toy_text]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44810274-248e-45e7-b6ae-26bf1b3aa285",
   "metadata": {},
   "source": [
    "On charge l'environnement Frozen Lake 8x8, qui se présente sous la forme d'une grille 8x8, dans laquelle on se déplace avec 4 actions (0: gauche, 1: bas, 2: droite, 3: haut). Cependant, le déplacement effectivement réalisé peut être perpendiculaire à celui attendu, car le lac est gelé et donc glissant: les transitions sont aléatoires. \n",
    "\n",
    "Les états (cases) sont numérotées de 0 à 63 selon l'expression suivante: ligne * 8 + colonne, où ligne et colonne vont de 0 à 7. La case en haut à gauche (0) de l'état de départ. La case en bas à droite (63) est l'objectif: une transition vers cet état donne un gain de 1. Toutes les autres transitions donnent un gain de 0. Lorsque cet état est atteint, on y reste et les paiements futurs sont nuls: on dit que l'épisode est terminé. Par ailleurs, il y certains états, notés H, qui sont des trous. Si on atteint un trou, on y reste et les paiements futurs sont nuls: l'épisode est terminé.\n",
    "\n",
    "On pourra lire la documentation https://gymnasium.farama.org/environments/toy_text/frozen_lake/ pour plus d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d0503c4-8846-4fe8-8433-21fa648e349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"FrozenLake8x8-v1\", render_mode=\"ansi\", is_slippery=True)\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a5220-ac01-48c6-ba9b-a57dd479f51a",
   "metadata": {},
   "source": [
    "Exécuter plusieurs fois la cellule suivante pour observer les états successifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d35e8d3-7765-46e3-bad5-7efdc299f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state, reward, terminated, truncated, info = env.step(1)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966661e-ffde-4e38-b7a4-80f17da761d2",
   "metadata": {},
   "source": [
    "La fonction `env.step()` ci-dessus renvoie 5 valeurs: `state` contient le nouvel état, `reward` le gain, `terminated` est un booléen qui indique si l'épisode est terminé, et `truncated` est un booléen qui indique si l'épisode a été terminé car trop long (par défaut, cet environnement termine l'épisode au bout de 200 étapes). La variable `info` contient des informations supplémentaires dont on ne se servira pas.\n",
    "\n",
    "Lorsqu'un épisode est terminé, il faut en redémarrer un de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f7660-9cb4-4534-af75-a64fe972a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dfabc-9ce8-4c1a-bbe7-7f0f9afec9f8",
   "metadata": {},
   "source": [
    "*Question 1*: Écrire une fonction qui prend en argument une politique, et qui l'utilise pendant un certain nombre d'épisodes avant d'indiquer la proportion de réussites (d'épisodes qui ont atteint l'état 63). Une politique sera donnée sous la forme d'une fonction qui à un état (0 à 63) associe une action (0 à 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c82caa-a7b2-4178-a3a5-a446712ee9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(policy,n_episodes=1000):\n",
    "   # Compléter\n",
    "    return n_success/n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd78e41-a089-4cc9-8c98-c069b76dc386",
   "metadata": {},
   "source": [
    "*Question 2*: Implémenter le Q-learning avec une politique epsilon-gloutonne. A intervalles régulier au fil des itérations, utiliser la fonction ci-dessus pour évaluer le taux de succès de politique gloutonne courante. Tracer ensuite le taux de succès en fonction du nombre d'itérations. On pourra adapter certaines fonctions implémentées dans le TP précédent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602dba96-7df9-4a82-ad00-58f765c268e3",
   "metadata": {},
   "source": [
    "*Question 3:* Essayer en faisant décroître epsilon au fil des itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3cc4e-62a7-4586-968e-6fd812db1a5d",
   "metadata": {},
   "source": [
    "*Question 4*: Mêmes questions pour SARSA (à T-étapes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
