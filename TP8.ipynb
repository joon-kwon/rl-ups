{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c50251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137db131",
   "metadata": {},
   "source": [
    "# Environnement Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7fafc",
   "metadata": {},
   "source": [
    "On travaille dans un premier temps avec l'environnement Cart Pole dont pourra consulter la documentation : https://gymnasium.farama.org/environments/classic_control/cart_pole/. Dans la pratique, on considère ce problème *résolu* lorsqu'on obtient une politique qui fait systématiquement durer les épisodes pendant 500 étapes. On souhaite le résoudre à l'aide de méthodes de gradient de politique et des réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591dc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74e491",
   "metadata": {},
   "source": [
    "La fonction suivante permet de visualiser le résultat d'une politique sur un épisode. La politique est donnée sous la forme d'une fonction prenant en argument un état et renvoyant une action (éventuellement tirée selon une distribuiton de probabilité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5408b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(pi):\n",
    "    local_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "    s = local_env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        a = pi(s)\n",
    "        s, r, terminated, truncated , _ = local_env.step(a)\n",
    "        clear_output(wait=True)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(local_env.render())\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9766581",
   "metadata": {},
   "source": [
    "**Question 1**: Implémenter et observer la politique qui pousse toujours vers la gauche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f8cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1405ce75",
   "metadata": {},
   "source": [
    "Voici un exemple de réseau de neurones dont l'entrée est de dimension 64, la sortie de dimension 8, avec deux couches cachées de tailles 32 et 16, où les fonctions d'activation sont ReLU, sauf la dernière qui est la fonction softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "455a1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, env.action_space.n),\n",
    "    nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26c0f0",
   "metadata": {},
   "source": [
    "**Question 2**: Définir une famille paramétrique de politiques, de type softmax, sous la forme d'un (petit) réseau de neurones. De quelles dimensions doivent être l'entrée et la sortie d'un tel réseau ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d644e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c4745c7",
   "metadata": {},
   "source": [
    "La fonction suivante prend en argument une politique (sous la forme d'un réseau de neurones) et un état, et qui renvoie une action tirée selon la distribution spécifiée par la politique dans cet été"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5b161f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action(pi, s):\n",
    "    probs = pi(torch.tensor(s))\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    a = dist.sample().item()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118c52f",
   "metadata": {},
   "source": [
    "Les algorithmes d'apprentissage par renforcement faisant intervenir des familles paramétriques sont initialement définies à l'aide d'itérations de type SGD pour mettre à jour les paramètres. On peut les adapter avec des variantes/sophistications de SGD réputées pour leur efficacité pratique, e.g. RMSprop ou Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "776c637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3 # à ajuster\n",
    "optimizer = optim.Adam(pi.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb49f3",
   "metadata": {},
   "source": [
    "## REINFORCE (sans baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c7029",
   "metadata": {},
   "source": [
    "Dans sa version la plus simple, l'algorithme REFINFORCE (sans baseline) met à jour les paramètres après chaque épisode comme suit (en reprenant les notations du polycopié):\n",
    "$$\\theta_{k+1}=\\theta_k+\\alpha\\sum_{t=0}^{+\\infty}\\gamma^t\\left( \\sum_{t'=1}^{+\\infty}\\gamma^{t'-1}R_{t+t',k} \\right) \\nabla_{\\theta}\\log \\pi_{\\theta_k}(A_{t,k}|S_{t,k}),\\qquad k\\geqslant 0.$$\n",
    "\n",
    "On note que la quantité (qu'on peut appeler *perte*) $\\sum_{t=0}^{+\\infty}\\gamma^t\\left( \\sum_{t'=1}^{+\\infty}\\gamma^{t'-1}R_{t+t',k} \\right) \\nabla_{\\theta}\\log \\pi_{\\theta_k}(A_{t,k}|S_{t,k})$ est l'opposé du gradient (par rapport à theta) de la quantité suivante $-\\sum_{t=0}^{+\\infty}\\gamma^t\\left( \\sum_{t'=1}^{+\\infty}\\gamma^{t'-1}R_{t+t',k} \\right)\\log \\pi_{\\theta_k}(A_{t,k}|S_{t,k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd3b1c",
   "metadata": {},
   "source": [
    "**Question 3**: Implémenter l'algorithe REINFORCE (sans baseline) et le faire tourner pendant 1000 épisodes. On utilisera la fonctionnalité de différentiation automatique de PyTorch. Tracer l'évolution du paiement cumulé de chaque épisode. Visualiser la politique obtenue. *Jouer sur le `learning_rate` et la structure du réseau de neurones pour obtenir un meilleur apprentissage (commencer par de très petits réseaux). Ne pas oublier de redéfinir le réseau de neurones à chaque nouvelle utilisation de l'algorithme, afin de ne pas garder les paramètres précédemment obtenus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir ici à nouveau le réseau de neurones\n",
    "\n",
    "n_episodes = 1000\n",
    "# compléter\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    s = env.reset()[0] # démarre un nouvel épisode\n",
    "    \n",
    "    # compléter\n",
    "    \n",
    "    terminated = False\n",
    "    truncated =  False\n",
    "    while not (terminated or truncated):\n",
    "        # compléter\n",
    "    \n",
    "    G = 0\n",
    "    loss = torch.tensor(0)\n",
    "    # Calculer dans 'loss' la valeur de la fonction de perte évoquée dans la question précédente\n",
    "    for s,a,r in zip(reversed(ss), reversed(aa), reversed(rr)):\n",
    "        # compléter\n",
    "\n",
    "    # Le code suivant calcule le gradient de la fonction de perte (par différentiation automatique), puis utilise ce dernier pour mettre à jour les paramètres du réseau de neurones.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculer la somme des paiements de l'épisode\n",
    "    episode_reward = # compléter\n",
    "    print(\"\\rEpisode {}/{}, Paiement cumulé: {}\".format(episode + 1, n_episodes, episode_reward), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232219ba",
   "metadata": {},
   "source": [
    "## REINFORCE (avec baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46473024",
   "metadata": {},
   "source": [
    "**Question 4**: Implémenter REFINFORCE (avec baseline). Comparer empiriquement les deux algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = # compléter\n",
    "\n",
    "v = # compléter\n",
    "\n",
    "# compléter\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    s = env.reset()[0] \n",
    "    \n",
    "    # compléter\n",
    "    while not (terminated or truncated):\n",
    "        # compléter\n",
    "    \n",
    "    G = 0\n",
    "    pi_loss = torch.tensor(0)\n",
    "    v_loss = torch.tensor(0)\n",
    "    for s,a,r in zip(reversed(ss), reversed(aa), reversed(rr)):\n",
    "        # compléter\n",
    "        v_loss = # compléter\n",
    "        pi_loss = # compléter\n",
    "\n",
    "    v_optimizer.zero_grad()\n",
    "    pi_optimizer.zero_grad()\n",
    "\n",
    "    pi_loss.backward(retain_graph=True)\n",
    "    v_loss.backward()\n",
    "\n",
    "    v_optimizer.step()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57655a",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38887d10",
   "metadata": {},
   "source": [
    "On considère une variante de l'algorithme acteur-critique vu en cours, où la mise à jour des paramètres s'écrit (dans sa version la plus simple, c'est-à-dire avec des itérations de type SGD) :\n",
    "$$\\Delta_{t,k}=R_{t+1,k}+\\gamma v_{w_{t,k}}(S_{t+1,k})-v_{w_{t,k}}(S_{t,k})$$\n",
    "$$\\theta_{t+1,k}=\\theta_{t,k}+\\alpha\\Delta_{t,k}\\nabla_{\\theta}\\log \\pi_{\\theta_{t,k}}(A_{t,k}|S_{t,k})$$\n",
    "$$w_{t+1,k}=w_{t,k}+\\beta \\Delta_{t,k}(\\gamma \\nabla_{w} v_{w_{t,k}}(S_{t+1,k})- \\nabla_{w} v_{w_{t,k}}(S_{t,k})),$$\n",
    "où on a repris les notations du polycopié.\n",
    "\n",
    "**Question 5**: Réécrire la mise à jour du paramètre $w$ à l'aide du gradient d'une certaine quantité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2c494",
   "metadata": {},
   "source": [
    "**Question 6**: Implémenter l'algorithme et observer les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66174ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b767d28b",
   "metadata": {},
   "source": [
    "# Environnement Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c60bc",
   "metadata": {},
   "source": [
    "Cet environnement est légèrement plus difficile à résoudre que le précédent. On pourra consulter la documentation ici : https://gymnasium.farama.org/environments/classic_control/acrobot/.\n",
    "Dans la pratique, on considère que ce problème est résolu lorsqu'on obtient une politique qui arrive presque toujours à terminer les épidodes en moins de 100 étapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acee3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "def render_policy(pi):\n",
    "    local_env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
    "    s = local_env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        a = pi(s)\n",
    "        s, r, terminated, truncated , _ = local_env.step(a)\n",
    "        clear_output(wait=True)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(local_env.render())\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80bcbb",
   "metadata": {},
   "source": [
    "**Question 7**: Résoudre ce problème à l'aide des algorithmes vus précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011c976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
